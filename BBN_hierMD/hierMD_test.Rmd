---
title: "Experiments with Hierarchical Multinomial Dirichlet Priors for the Conditional Probability Tables of Discrete Bayesian Networks"
author: "Michael L. Thompson"
date: "11/23/2020"
output: 
  pdf_document:
    toc: yes
    toc_depth: 4
linkcolor: red
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 5
)
```

\newpage

## Introduction

This is a brief experiment with Hierarchical Multinomial-Dirichlet priors for Conditional Probability Tables (CPTs) of discrete Bayesian Networks (i.e., Bayesian belief networks, BBN).

I demonstrate the following two approaches:

1. **`hierMD`** -- The base case proposed by L. Azzimonti, G. Corani, and M. Zaffalon (*ACZ*) of [Imprecise Probability Group @ IDSIA](https://ipg.idsia.ch/) in: ["Hierarchical
    estimation of parameters in Bayesian networks"](https://www.sciencedirect.com/science/article/pii/S0167947319300519?casa_token=4457Rva4KqUAAAAA:0Q2kWKJA1hBADWxX6CKgJxb8QxRVV2x6v2iQdmM9Lin9-hJ6AX9PG8qa4KT35iIH4dL2YMhR75w), (**I show the Stan code I adapted from [their code]().**)          
2. **`hierMDmix`** -- A mixture model of parent states that I've proposed. This approach mixes the Dirichlet parameters of each parent's state -- i.e., the "local Dirichlet prior" -- with those of the others to form a single parameter vector.  These local-level priors each depend upon the populaton-level prior similar to how the single columnwise priors of the *ACZ* approach do.
 

 My Conclusions (w/out having done any other testing than what's shown below):
 
 * The more complicated mixture model gives CPT results that are less pulled towards the marginal distribution of the child node than does the *ACZ* model.
 * In the resulting inferences, both approaches are an improvement over a flat prior -- the traditional `BDeu` (*Bayesian Dirichlet equivalent uniform* or *Laplace smoothing*) approach.

## Prepare the R Environment

Here, we load the packages we need.

```{r pkgs}
library(magrittr) # I always use piping!
library(tidyverse) # Thank heaven & Hadley Wickham for the `tidyverse`!

# Bayesian network packages
library(bnlearn)
library(gRain)
# Implementation of **Stan** probabilistic programming language
library(rstan)
rstan_options(auto_write = TRUE)

select <- dplyr::select

```


This section includes a hidden **R** code chunk that defines functions we need (rather than `source`-ing a script).  View this R Markdown document's source to see the functions.

```{r fncs, include=FALSE}

# *** Function gen_CPT_hierMD ***
gen_CPT_hierMD <- function(bbn, df, ch_name = "gender", full = FALSE){
  # Let's first do the "gender" node. It has 2 parents: "Str.ctr_79" and "occupation".
  #ch_name <- "gender"
  cpt <- bbn$cptlist[[ch_name]] # array with CPT using BDeu priors (Laplace smoothing.)
  # Build data list for Stan program sm_hierMD
  n_st_ch  <- dim(cpt)[[1]]
  pr_names <- setdiff( names(dim(cpt)), ch_name )
  df_ch_pr <- df %>%
    count(across(all_of(c(rev(pr_names),ch_name))),.drop = FALSE) %>%
    unite(col = "pr_combo", all_of(pr_names), sep = "#")
  N_ch_pr <- matrix(df_ch_pr$n, nrow = n_st_ch)
  
  theta_BDeu1    <- t(N_ch_pr + (1/n_st_ch)) %>% divide_by(rowSums(.))
  # These should match "cpt"
  theta_BDeu_eps <- t(N_ch_pr + (1/length(N_ch_pr))) %>% divide_by(rowSums(.))
  # spot-check...
  #(cpt[,,1,drop=FALSE]) %>% round(3)
  #theta_BDeu_eps %>% head(n_st_pr_i[[1]]) %>% t() %>% round(3)
  #(cpt[,,15,drop=FALSE]) %>% round(3)
  #theta_BDeu_eps[(15-1)*n_st_pr_i[[1]] + (1:n_st_pr_i[[1]]),] %>% t() %>% round(3)
  
  data_list_hierMD <- list(
    n_st_ch = n_st_ch, # number of child states
    n_st_pr = prod(dim(cpt)[-1]), # number of total combos of parent states
    N_ch_pr = N_ch_pr, # number of cases at all combos of parents & child
    alpha_0 = array(rep(1,n_st_ch),dim=n_st_ch) # hyperparameter for Dirichlet priors
  )
  
  n_st_pr_i <- {dim(cpt)[-1]} %>% array(dim=length(.))
  data_list_hierMDmix <- c(
    data_list_hierMD,
    list(
      n_parent  = length(pr_names), # number of parents
      n_st_pr_i = n_st_pr_i, # number of states for each parent
      # state of each parent for each combo of parents:
      i_st_pr   = as.matrix(expand.grid(purrr::map(n_st_pr_i,~seq(1,.x)))) 
    )
  )
  
  # Estimate posterior of CPT for child node:
  # 1. hierMD
  vb_rng_seed <- 42
  sf_hierMD   <- vb( sm_hierMD, data = data_list_hierMD, seed = vb_rng_seed )
  rslt_hierMD <- rstan::extract(sf_hierMD)
  names(rslt_hierMD)
  theta_hierMD <- rslt_hierMD$theta %>% apply(2:length(dim(.)),mean)
  # 2. hierMDmix
  sf_hierMDmix <- vb(sm_hierMDmix,data=data_list_hierMDmix,seed= vb_rng_seed)
  rslt_hierMDmix <- rstan::extract(sf_hierMDmix)
  names(rslt_hierMDmix)
  theta_hierMDmix <- rslt_hierMDmix$theta %>% apply(2:length(dim(.)),mean)
  
  result <- list(
    theta_BDeu1     = theta_BDeu1,
    theta_BDeu_eps  = theta_BDeu_eps,
    theta_hierMD    = theta_hierMD,
    theta_hierMDmix = theta_hierMDmix
  )
  if(full){
    result <- c(
      result,
      list(
        rslt_hierMD    = rslt_hierMD,
        rslt_hierMDmix = rslt_hierMDmix,
        data_list_hierMD = data_list_hierMD,
        data_list_hierMDmix = data_list_hierMDmix
      )
    )
  }
  invisible( result)
}

# *** FUNCTION revise_bbn ***
# This function loads the CPT parameters estimated using the alternative
# approaches into the original BBN
revise_bbn <- function(bbn, ch_name, theta_hierMD, theta_hierMDmix){
  bbn_hierMD <- bbn$hierMD
  bbn_hierMD$cptlist[[ch_name]] <- array(
    t(theta_hierMD),
    dim=dim(cpt),
    dimnames=dimnames(cpt)
  )
  bbn_hierMD <- compile(bbn_hierMD)
  # Revise BBN using CPT from hierMDmix
  bbn_hierMDmix <- bbn$hierMDmix
  bbn_hierMDmix$cptlist[[ch_name]] <- array(
    t(theta_hierMDmix),
    dim=dim(cpt),
    dimnames=dimnames(cpt)
  )
  bbn_hierMDmix <- compile(bbn_hierMDmix)
  return( list(hierMD = bbn_hierMD, hierMDmix = bbn_hierMDmix) )
}

# *** FUNCTION gbf ***
gbf <- function(bbn, gndr = "F",occ = "doctor",age = "yrs_33_44"){
  # Generalized Bayes Factors: O(H|E)/O(H) -- posterior-to-prior odds-ratio
  # (all conditioned upon Has_Seen = "yes")
  joint <- bbn %>%
    querygrain(
      nodes = c("gender","occupation","age"),
      evidence = list(Has_Seen = "yes" ),
      type  = "joint"
    )
  
  prior_odds_F_occ <- joint %>% {./(1 - .)}
  
  joint_post <- bbn %>%
    querygrain(
      nodes = c("gender","occupation","age"),
      evidence=list(LIKE = "yes"),
      type="joint"
    )
  posterior_odds_F_occ_like <-  joint_post %>% {./(1-.)}
  
  # H: {gender,occupation,age}, E: {like movie} (all given Has_Seen="yes")
  GBF_HE <- posterior_odds_F_occ_like / prior_odds_F_occ
  
  full <- list(
    gbf        = GBF_HE,
    wte        = 10*log10(GBF_HE), # in decibans
    joint      = joint,
    joint_post = joint_post,
    prior_odds = signif(prior_odds_F_occ,3),
    post_odds  = signif(posterior_odds_F_occ_like,3)
  )
  at_cond <- lapply( full, function(x){x[age,occ,gndr]} )
  invisible( list( at_cond = at_cond, full = full ) )
}



```


## Load the Bayesian Belief Network (BBN)

We'll use a BBN that I've generated before. The network is one of many in an ensemble of BBNs used to build a Movie Recommender System  -- see my presentation at the [8th Annual BayesiaLab Conference (2020)](https://library.bayesia.com/articles/#!bayesialab-knowledge-hub/2020-conference-michael-thompson) and the code and PDF of the slides at this github repository, ["Bayesian Analysis"](https://github.com/apollostream/Bayesian-Analysis).

The particular BBN shown here predicts the viewer's rating for the movie "Star Trek: The Motion Picture" (1979) given any combination of viewer features -- age, gender, or occupation -- movie genre, and ratings of other movies. (It was based on 694 observations in the smallest version of the [MovieLens dataset](https://grouplens.org/datasets/movielens/).)


```{r netload, results='hide'}
# DATA LOADING ====
# Get a BBN to experiment with.
# Use it to simulate data, too.
bbn <- loadHuginNet(
  file = "bbn_StarTrek_79.net", 
  description = "Predicts ratings of 'Star Trek: The Motion Picture (1979)'"
)
```

### Visualize the BBN

```{r vizbbn}
# plot it
g1 <- bbn$dag  %>%
  as.bn() %>%
  graphviz.plot( render = FALSE )
g1 %>% 
  plot( 
    attrs = list(node=list(fontsize="32")), 
    main = 'Star Trek: The Motion Picture (1979)'
  )
```

### Perform Test Inferences

Just to demonstrate some of the nature of the BBN we're experimenting with, we perform a few inferences on the base BBN.

We plan on concentrating on inferences between the viewer features (`age`, `gender`, `occupation`) and the viewing (`Has_Seen`), liking (`LIKE`), and rating (`Str.ctr_79`) nodes of the BBN.

```{r test_inference}
# The movie nodes have states that are in 1-point deviations from a viewer's
# median rating over all movies the viewer has seen. Assume dev. of zero or
# greater means viewer liked the movie.
movie_node <- "Str.ctr_79"
querygrain(bbn,"Has_Seen") %>% map(round,3)
querygrain(bbn,"LIKE") %>% map(round,3)
querygrain(bbn,"LIKE", evidence = c(Has_Seen="yes") ) %>% map(round,3)
querygrain(bbn,c("LIKE","Has_Seen"),type="conditional") %>% round(3)
querygrain(bbn,nodes = c("Has_Seen","age"),type="conditional") %>% round(3)
querygrain(
  bbn,
  nodes = c(movie_node,"age"), 
  evidence = c(Has_Seen="yes"), 
  type="conditional"
) %>% round(3)
```

### Generate Data

We'll also use the BBN to generate data that will serve as the bases for traditionally Laplace-smoothed parameter estimates and to see how sparsely the data lie in this high-dimensional discrete-variate space.

```{r gen_data}
rng_seed <- 31
N   <- 1000L
df  <- bbn %>%
  simulate( nsim = N, seed = rng_seed ) %>%
  as_tibble()
```

## Code in the **Stan** Probabilistic Programming Language

We use package `rstan`, based upon the **Stan** probabilistic programming language.  Here, we show the programs, one each for the hierarchical Multinomial Dirichlet approach of *ACZ*, and the mixture variant that I propose. The code below shows how straight-forward it is to implement these approaches in **Stan**. The `hierMD` code is less than 25 lines!

(Although it is possible to have `knitr` execute the code compilation directly from the **Stan** code chunks, we'll forgoe that and compile the programs from their respective files on disk in a later **R** code chunk.)

### `hierMD` **Stan** Program

This first **Stan** program is adapted from the original by *ACZ*: I've copied the code from my file `hierMD_MLT.stan` without its banner comments acknowledging Azzimonti *et al.* -- see my **Stan** files `hierMD_MLT.stan` and `hierMDmix_MLT.stan` for the full banners. This version of *ACZ*'s approach adds a proper likelihood statement and puts a prior on the magnitude of the Dirichlet parameter, `N_prior`, which in the original code was a fixed input value `s`.  I've also dropped the estimation of the posterior of the marginal probability distribution parameters for the parent states (`thetaY` in the original *ACZ* code).


```{stan exmpl, output.var = "sm_dummy", eval=FALSE}
data {
  int<lower=2> n_st_ch; // number of child states
  int<lower=2> n_st_pr; // number of total combos of parent states
  int<lower=0> N_ch_pr[n_st_ch,n_st_pr]; // number of cases at all combos of parents & child
  vector<lower=0>[n_st_ch] alpha_0; // hyperparameter for Dirichlet priors
}
parameters {
  simplex[n_st_ch] theta[n_st_pr]; // conditional probability table parameters
  simplex[n_st_ch] alpha_norm; // population-level parameter for Dirichlet priors, normalized
  real<lower=0> N_prior; // number of cases represented by prior
}
transformed parameters {
  vector<lower=0>[n_st_ch] alpha; // population-level parameter for Dirichlet priors
  alpha = N_prior * alpha_norm;
}
model {
  alpha_norm ~ dirichlet(alpha_0); // prior
  N_prior    ~ student_t(4,1,1);   // prior
  for (i_st_pr in 1:n_st_pr){
    theta[i_st_pr]    ~ dirichlet( alpha ); // prior
    N_ch_pr[,i_st_pr] ~ multinomial( theta[i_st_pr] ); // likelihood
  }
}

```

### `hierMDmix` **Stan** Program

And, here's the **Stan** implementation of my mixture variant, `hierMDmix`. (See file `hierMDmix_MLT.stan` for the full banner of acknowledgments to *ACZ*.)

```{stan exmplmix, output.var = "sm_dummymix", eval=FALSE}
data {
  int<lower=2> n_st_ch; // number of child states
  int<lower=2> n_st_pr; // number of total combos of parent states
  int<lower=0> N_ch_pr[n_st_ch,n_st_pr]; // number of cases at all combos of parents & child
  vector<lower=0>[n_st_ch] alpha_0; // hyperparameter for Dirichlet priors
  int <lower=1> n_parent; // number of parents
  int <lower=2> n_st_pr_i[n_parent]; // number of states for each parent
  int i_st_pr[n_st_pr,n_parent]; // state of each parent for each combo of parents
}
transformed data {
  int n_st_pr_sum; // sum of number of states for parents
  vector[n_parent] alpha_pr_mix; // Dirichlet parameters for mixture
  n_st_pr_sum  = sum(n_st_pr_i);
  alpha_pr_mix = rep_vector(1,n_parent);
}
parameters {
  simplex[n_st_ch] theta[n_st_pr];  // conditional probability table parameters
  simplex[n_st_ch] alpha_norm; // population-level parameter for Dirichlet priors, normalized
  simplex[n_st_ch] alpha_i_norm[n_st_pr_sum];// prior for the local states
  real<lower=0> N_prior;  // number of cases represented by prior
  simplex[n_parent] p_mix; // mixture probabilities on the parents alpha_i
}
transformed parameters {
  vector<lower=0>[n_st_ch] alpha_ch[n_st_pr]; // mixture alpha
  vector<lower=0>[n_st_ch] alpha; // population level
  vector<lower=0>[n_st_ch] alpha_i[n_st_pr_sum];// prior for local states
  alpha = N_prior * alpha_norm;  // population-level
  for(i in 1:n_st_pr_sum){ alpha_i[i] = N_prior*alpha_i_norm[i];}// parent state-level
  // Mix alpha hyperparameter for the prior over each parent's state
  for( i_st in 1:n_st_pr ){
    alpha_ch[i_st] = rep_vector(0,n_st_ch); // initialize as zeros
    for( i in 1:n_parent ){
      // cummulative mixture contributions of parent states
      alpha_ch[i_st] += p_mix[i] * alpha_i[ sum(head(n_st_pr_i,i-1)) + i_st_pr[i_st,i] ];
    }
  }
}
model {
  alpha_norm ~ dirichlet( alpha_0 );      // prior
  N_prior    ~ student_t( 4, 1, 1 );      // prior
  p_mix      ~ dirichlet( alpha_pr_mix ); // prior
  {
    int i_st = 0;
    for( i in 1:n_parent ){
      for( j in 1:n_st_pr_i[i]){
        i_st += 1;
        alpha_i_norm[i_st] ~ dirichlet( alpha ); // prior
      }
    }
  }
  for (i_st in 1:n_st_pr){
    theta[i_st]    ~ dirichlet( alpha_ch[i_st] ); // prior
    N_ch_pr[,i_st] ~ multinomial( theta[i_st] );  // likelihood
  }
}
```

Now, we compile the **Stan** programs from file.

```{r stan_cmpl}
# STAN COMPILATION ====
# Compile the Stan programs of the two model variants.
sm_hierMD    <- stan_model(file="hierMD_MLT.stan",    model_name="hierMD")
sm_hierMDmix <- stan_model(file="hierMDmix_MLT.stan", model_name="hierMDmix")
```

## Estimation of the Conditional Probability Tables (CPT)

We show application of the `gen_CPT_hierMD()` function, which performs Variational Bayesian Infernce on both models to generate CPT parameters first using the `hierMD` method then the `hierMDmix` method for a single named child node.  It also computes the parameters using Laplace smoothing (`BDeu`).

We could do it for every child node in the BBN, but instead, we're just going
to do it on the three viewer feature nodes: `gender`, `age`, and `occupation`.

```{r cpt_estmtn, results='hide'}
# CPT ESTIMATION ====
rslt_gender <- gen_CPT_hierMD( bbn, df, ch_name = "gender" )
rslt_age    <- gen_CPT_hierMD( bbn, df, ch_name = "age" )
rslt_occ    <- gen_CPT_hierMD( bbn, df, ch_name = "occupation" )

rslt <- list(gender=rslt_gender, age = rslt_age, occupation = rslt_occ)

# # Do All Child Nodes:
# rslt <- setdiff( bbn$universe$nodes, movie ) %>%
#   set_names(.,.) %>%
#   map( ~ gen_CPT_hierMD(bbn, df, ch_name = .x) )
```


Display the CPT parameters of the `gender` node for each method.

```{r theta}
# Test the gender-node revision
theta_BDeu_eps  <- rslt_gender$theta_BDeu_eps
theta_hierMD    <- rslt_gender$theta_hierMD
theta_hierMDmix <- rslt_gender$theta_hierMDmix

df %>% 
  filter(str_detect(occupation,"programmer")) %$% 
  table(gender,Str.ctr_79) %>% 
  {list(`OCCURRENCES: occupation == "programmer"`= .)}

cpt <- bbn$cptlist$gender
n_st_pr_i <- dim(cpt)[-1]

# The 15th occupation is "programmer".
(cpt[,,15,drop=FALSE]) %>% round(3)
theta_BDeu_eps[(15-1)*n_st_pr_i[[1]] + (1:n_st_pr_i[[1]]),] %>% 
  t() %>% 
  round(3) %>%
  {dimnames(.)<- dimnames(cpt)[-3]; .}
theta_hierMD[(15-1)*n_st_pr_i[[1]] + (1:n_st_pr_i[[1]]),] %>% 
  t() %>% 
  round(3) %>%
  {dimnames(.)<- dimnames(cpt)[-3]; .}
theta_hierMDmix[(15-1)*n_st_pr_i[[1]] + (1:n_st_pr_i[[1]]),] %>% 
  t() %>% 
  round(3) %>%
  {dimnames(.)<- dimnames(cpt)[-3]; .}
```


## Revision of the BBN

Now, let's load the CPTs into separate BBN and evaluate how they differ from the original BBN in terms of inferences.

```{r revbbn}
# REVISION OF BBN ====
bbn_list <- list( hierMD=bbn, hierMDmix=bbn )
for(ch_name in c("age","gender","occupation")){
  
  bbn_list <- revise_bbn(
    bbn_list, 
    ch_name         = ch_name, 
    theta_hierMD    = rslt[[ch_name]]$theta_hierMD, 
    theta_hierMDmix = rslt[[ch_name]]$theta_hierMDmix 
  )
}

bbn_hierMD    <- bbn_list$hierMD
bbn_hierMDmix <- bbn_list$hierMDmix
```

## Impact on Bayesian Inference

The space is so high-dimensional, and we only have `N=1000` cases.  So, most of the combos are not measured. Yet, the models will make inferences for any combination of node values.

(What's actually needed is feedback to the practitioner that the network is highly uncertain about any inferences in such instances.  That's where having the full posterior distributions of the CPT parameters is helpful. But, we don't explore that here.)

```{r infer1a}
# IMPACT ON BAYESIAN INFERNCE ====
# Conditional ratings distribution and expected rating given gender,
# posterior given a programmer who has seen the movie.
 
# First, show occurrences:
df %>% 
  filter(
    str_detect(occupation,"programmer"),
    str_detect(Has_Seen,"yes"),
    str_detect(age, "yrs_33_44")
  ) %$% 
  table(Str.ctr_79,gender) %>% 
  {list(`OCCURRENCES: `= .)}

case_profile <- list(occupation="programmer", Has_Seen = "yes", age = "yrs_33_44")
querygrain(bbn,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>% 
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```

```{r infer1b}
querygrain(bbn_hierMD,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>% 
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```

```{r infer1c}
querygrain(bbn_hierMDmix,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>% 
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```

Now, let's look at a scenario with no data: that of `occupation = "doctor"`.

```{r infer2a}
# Conditional ratings distribution given gender,
# posterior given a doctor who has seen the movie.
# First, show occurrences:
df %>% 
  filter(
    str_detect(occupation,"doctor"),
    str_detect(Has_Seen,"yes"),
    str_detect(age, "yrs_33_44")
  ) %$% 
  table(Str.ctr_79,gender) %>% 
  {list(`OCCURRENCES: `= .)}

case_profile <- list(occupation="doctor", Has_Seen = "yes", age = "yrs_33_44")
querygrain(bbn,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>% 
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```


```{r infer2b}
querygrain(bbn_hierMD,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>% 
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```


```{r infer2c}
querygrain(bbn_hierMDmix,
           nodes = c("Str.ctr_79","gender"), 
           evidence = case_profile,
           type = "conditional") %T>% {print(signif(.,3))} %>%
  {list(Expected_Rating = round(t(.) %*% c(-3:2,0),2))}
```

## Impact on Generalized Bayes Factor, GBF(H:E)

The Generalized Bayes Factor, GBF(H:E), has been shown to be a good metric for hypothesis (H) confirmation given evidence (E) and for use in generating relevant explanations  (a ranked list of H's) of observed evidence (E). However, it is very sensitive to noisy estimates of prior and posterior probabilities for sparsely measured cases.

Given that the `hierMD` and `hierMDmix` approaches both spread probability mass throughout the sparse CPTs that is more consistent with the population-level distributions, we would expect the GBF(H:E) for cases of either sparsely measured hypotheses H or evidence E to be less noisy, though somewhat biased due to the shrinkage towards the population marginal distributions that these priors induce.

```{r gbf1a}
# IMPACT ON GENERALIZED BAYES FACTOR ====
# Compare models impact on GBF(H:E) under different case profiles as
# the "hypothesis" H, given the "evidence" E = LIKE = "yes".
list( BDeu = bbn, hierMD = bbn_hierMD, hierMDmix = bbn_hierMDmix ) %>%
  imap_dfr(
    ~ gbf(bbn = .x, gndr = "F", occ = "doctor", age = "yrs_33_44") %$% 
      map(at_cond,signif,3) %>%
      as_tibble() %>% 
      mutate(model=.y) %>% 
      select(model,everything())
  )
```

```{r gbf1b}
list( BDeu = bbn, hierMD = bbn_hierMD, hierMDmix = bbn_hierMDmix ) %>%
  imap_dfr(
    ~ gbf(bbn = .x, gndr = "M", occ = "programmer", age = "yrs_33_44") %$% 
      map(at_cond,signif,3) %>%
      as_tibble() %>% 
      mutate(model=.y) %>% 
      select(model,everything())
  )
```

```{r gbf1c}
list( BDeu = bbn, hierMD = bbn_hierMD, hierMDmix = bbn_hierMDmix ) %>%
  imap_dfr(
    ~ gbf(bbn = .x, gndr = "F", occ = "programmer", age = "yrs_33_44") %$% 
      map(at_cond,signif,3) %>%
      as_tibble() %>% 
      mutate(model=.y) %>% 
      select(model,everything())
  )
```


```{r gbf1d}
list( BDeu = bbn, hierMD = bbn_hierMD, hierMDmix = bbn_hierMDmix ) %>%
  imap_dfr(
    ~ gbf(bbn= .x, gndr = "F", occ = "administrator", age = "yrs_33_44") %$% 
      map(at_cond,signif,3) %>%
      as_tibble() %>% 
      mutate(model=.y) %>% 
      select(model,everything())
  )
```


## Thoughts

Of course, much more work would be needed to show if and when the `hierMDmix` method adds value over that of the `hierMD` method.  But, *ACZ* have already shown the value of `hierMD` over traditional smoothing calculation of Bayesian network CPTs.

It is nice to see that either of these approaches is so easily implemented using **Stan**.  Moreover, *ACZ* provide **R** source code -- ["Hierarchical BN parameter estimation"](https://ipg.idsia.ch/software.php?id=139) -- to perform the variational Bayesian inference for `hierMD` both with and without using **Stan**.

Finally, having the full posterior (approximately) of the CPT parameters is a nice feature of these two methods.  In the future, we should exploit this by reporting or visualizing the uncertainty quantification in risk assessment and decision analysis.

## About

-Michael L. Thompson,

[*LinkedIn profile*](https://www.linkedin.com/in/mlthomps)


